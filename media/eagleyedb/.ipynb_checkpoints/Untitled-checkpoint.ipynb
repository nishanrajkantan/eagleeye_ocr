{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "121db13d-a403-445f-9f0b-07dbe94fe7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "85eed788-9f36-479e-a20e-ea5e19602e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output2_maybank_statement1.xlsx',\n",
       " 'output2_maybank_statement2.xlsx',\n",
       " 'output2_maybank_statement3.xlsx',\n",
       " 'output2_maybank_statement4.xlsx']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = glob.glob1('./', '*.xlsx')\n",
    "file_list"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf711325-a033-4213-b8f4-abbcee091fe0",
   "metadata": {},
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# Read the Output Template\n",
    "document_df = pd.read_excel         (file_list[0], sheet_name='Documents')\n",
    "entity_type_df = pd.read_excel      (file_list[0], sheet_name='EntityTypes')\n",
    "entities_df = pd.read_excel         (file_list[0], sheet_name='Entities')\n",
    "entity_attributes_df = pd.read_excel(file_list[0], sheet_name='EntityAttributes')\n",
    "entity_mentions_df = pd.read_excel  (file_list[0], sheet_name='EntityMentions')\n",
    "event_types_df = pd.read_excel      (file_list[0], sheet_name='EventTypes')\n",
    "event_mentions_df = pd.read_excel   (file_list[0], sheet_name='EventMentions')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd59b57e-3af7-4239-b336-eac4f8813cee",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "status = []\n",
    "def detect_outlier(data_1):\n",
    "    \n",
    "    threshold=1\n",
    "    mean_1 = np.mean(data_1)\n",
    "    std_1 =np.std(data_1)\n",
    "    \n",
    "    for y in data_1:\n",
    "        z_score= (y - mean_1)/std_1 \n",
    "        if np.abs(z_score) > threshold:\n",
    "            status.append('FRAUD')\n",
    "        else:\n",
    "            status.append('NORMAL')\n",
    "    return status\n",
    "\n",
    "datalake_df['TRANSACTION STATUS'] = detect_outlier(datalake_df['JUMLAH URUSNIAGA'])\n",
    "datalake_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "a4dfb798-539f-40b2-b6a5-eed0a49c99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist]\n",
    "\n",
    "\n",
    "def get_entity_type_id(datalake_df):\n",
    "    entity_type_id = []\n",
    "    for i in range(len(datalake_df.columns)):\n",
    "        column_index_no = datalake_df.columns.get_loc(datalake_df.columns[i]) + 1\n",
    "        entity_type_id.append(column_index_no)\n",
    "    return entity_type_id\n",
    "\n",
    "\n",
    "def get_entity_type(datalake_df):\n",
    "    datalake_df.columns = map(str.upper, datalake_df.columns)\n",
    "    entity_type = datalake_df.columns.to_list()\n",
    "    return entity_type\n",
    "\n",
    "\n",
    "def get_entity_desc(entity_type):\n",
    "    entity_type_desc = entity_type.copy()\n",
    "    return entity_type_desc\n",
    "\n",
    "\n",
    "def get_entity(datalake_df):\n",
    "    entities = flatten(datalake_df.values.tolist())\n",
    "    return entities\n",
    "\n",
    "\n",
    "def get_entity_id(entities):\n",
    "    entity_ids = []\n",
    "    for i in range(len(entities)):\n",
    "        entity_id = i + 1\n",
    "        entity_ids.append(entity_id)\n",
    "    return entity_ids\n",
    "\n",
    "\n",
    "def get_factor(entity_id, entity_type):\n",
    "    factor = len(entity_id) / len(entity_type)\n",
    "    return int(factor)\n",
    "\n",
    "\n",
    "def get_factor2(entity_id):\n",
    "    factor = len(entity_id)\n",
    "    return int(factor)\n",
    "\n",
    "\n",
    "def get_entity_type2(entity_id, entity_type_id):\n",
    "    entity_type = entity_type_id.copy()\n",
    "    factor = get_factor(entity_id, entity_type)\n",
    "    entity_type = entity_type_id.copy() * factor\n",
    "    return entity_type\n",
    "\n",
    "\n",
    "def get_location(factor):\n",
    "    locations = ['' for i in range(factor)]\n",
    "    return locations\n",
    "\n",
    "\n",
    "def get_address(factor):\n",
    "    addresses = ['' for i in range(factor)]\n",
    "    return addresses\n",
    "\n",
    "\n",
    "def get_image(entity_type):\n",
    "    images = []\n",
    "    image = ''\n",
    "    for i in range(len(entity_type)):\n",
    "        if entity_type[i] == 1:\n",
    "            image = '/assets/images/money.png'\n",
    "        elif entity_type[i] == 2:\n",
    "            image = '/assets/images/person.png'\n",
    "        elif entity_type[i] == 3:\n",
    "            image = '/assets/images/person.png'\n",
    "        else:\n",
    "            pass\n",
    "        images.append(image)\n",
    "    return images\n",
    "\n",
    "\n",
    "def get_doc_id(factor):\n",
    "    doc_ids = [1 for i in range(factor)]\n",
    "    return doc_ids\n",
    "\n",
    "\n",
    "def get_search_around(entity_type):\n",
    "    searcharounds = []\n",
    "    for i in range(len(entity_type)):\n",
    "        if entity_type[i] == 1:\n",
    "            searcharound = ''\n",
    "        elif entity_type[i] == 2:\n",
    "            searcharound = ''\n",
    "        elif entity_type[i] == 3:\n",
    "            searcharound = ''\n",
    "        searcharounds.append(searcharound)\n",
    "    return searcharounds\n",
    "\n",
    "\n",
    "def get_search_around_url(factor):\n",
    "    search_around_url = ['' for i in range(factor)]\n",
    "    return search_around_url\n",
    "\n",
    "\n",
    "def get_event_name():\n",
    "    event_name = ['send', '']\n",
    "    return event_name\n",
    "\n",
    "\n",
    "def get_event_desc(event_name):\n",
    "    event_desc = event_name.copy()\n",
    "    return event_desc\n",
    "\n",
    "\n",
    "def get_event_id(event_name):\n",
    "    event_ids = []\n",
    "    for i in range(len(event_name)):\n",
    "        event_id = i + 1\n",
    "        event_ids.append(event_id)\n",
    "    return event_ids\n",
    "\n",
    "\n",
    "def random_dates(start, end, n=1):\n",
    "    start_u = start.value // 10 ** 9\n",
    "    end_u = end.value // 10 ** 9\n",
    "    return pd.to_datetime(np.random.randint(start_u, end_u, n), unit='s')\n",
    "\n",
    "\n",
    "def get_doc_id2(entity_id):\n",
    "    docids = [1 for i in range(len(entity_id))]\n",
    "    return docids\n",
    "\n",
    "\n",
    "def get_entity_id2(entity_id):\n",
    "    entityids = entity_id.copy()\n",
    "    return entityids\n",
    "\n",
    "\n",
    "def get_start_idx(entity_id):\n",
    "    start_idx = [0 for i in range(len(entity_id))]\n",
    "    return start_idx\n",
    "\n",
    "\n",
    "def get_end_idx(entity_id):\n",
    "    end_idx = [5 for i in range(len(entity_id))]\n",
    "    return end_idx\n",
    "\n",
    "\n",
    "def get_date(dates_df):\n",
    "    dates_list = []\n",
    "    for i in range(len(dates_df)):\n",
    "        date = dates_df['TARIKH MASUK'][i]\n",
    "        for j in range(3):\n",
    "            dates_list.append(date)\n",
    "    return dates_list\n",
    "\n",
    "def get_date2(dates_df):\n",
    "    dates_list = []\n",
    "    for i in range(len(dates_df)):\n",
    "        date = dates_df['TARIKH MASUK'][i]\n",
    "        for j in range(3-1):\n",
    "            dates_list.append(date)\n",
    "    return dates_list\n",
    "\n",
    "def get_entity_unique_id(docid, entity_id, start_idx, end_idx):\n",
    "    df = pd.DataFrame([docid, entity_id, start_idx, end_idx]).T\n",
    "    df.columns = ['docid', 'entityid', 'startidx', 'endidx']\n",
    "    df['entityuniqueid'] = ''\n",
    "    for i in range(len(df)):\n",
    "        df['entityuniqueid'][i] = int(\n",
    "            str(df['docid'][i]) + str(df['entityid'][i]) + str(df['startidx'][i] + df['endidx'][i]))\n",
    "    entityuniqueid = df['entityuniqueid'].to_list()\n",
    "    return entityuniqueid\n",
    "\n",
    "\n",
    "def get_relations(entity_df, entity_mentions_df):\n",
    "    merged_df = entity_df.merge(entity_mentions_df, left_on='EntityId', right_on='EntityId')\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------\n",
    "    \n",
    "    # Replace entityuniqueid of similar entities\n",
    "    filtered_df = merged_df[~merged_df['EntityType'].isin([6, 9, 10, 11, 12])].dropna(subset=['EntityName'])\n",
    "    duplicated_df2 = filtered_df[filtered_df.duplicated(subset=['EntityName'])]\n",
    "    sorted_df2 = duplicated_df2.sort_values(ascending=True, by='EntityName').reset_index(drop=True)\n",
    "    for j in range(1, len(sorted_df2)):\n",
    "        if sorted_df2['EntityName'][j] == sorted_df2['EntityName'][j - 1]:\n",
    "            sorted_df2['EntityUniqueId'][j] = sorted_df2['EntityUniqueId'][j - 1]\n",
    "        else:\n",
    "            sorted_df2['EntityUniqueId'][j] = sorted_df2['EntityUniqueId'][j]\n",
    "    for i in range(len(sorted_df2)):\n",
    "        for j in range(len(merged_df)):\n",
    "            if sorted_df2['EntityName'][i] == merged_df['EntityName'][j]:\n",
    "                merged_df['EntityUniqueId'][j] = sorted_df2['EntityUniqueId'][i]\n",
    "    \n",
    "    # Split entity by entity id\n",
    "    entity_type1_df = merged_df[merged_df['EntityType'] == 1].reset_index(drop=True)\n",
    "    entity_type2_df = merged_df[merged_df['EntityType'] == 2].reset_index(drop=True)\n",
    "    entity_type3_df = merged_df[merged_df['EntityType'] == 3].reset_index(drop=True)\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------\n",
    "    \n",
    "    # Map Relations based on their entity type\n",
    "    entity1ids = []\n",
    "    entity2ids = []\n",
    "    entity1dtypes = []\n",
    "    entity2dtypes = []\n",
    "    \n",
    "    entity1d = entity_type2_df['EntityUniqueId'].tolist()\n",
    "    entity1dtype = entity_type2_df['EntityType'].tolist()\n",
    "    entity1ids.extend(entity1d)\n",
    "    entity1dtypes.extend(entity1dtype)\n",
    "    entity2d = entity_type1_df['EntityUniqueId'].tolist()\n",
    "    entity2dtype = entity_type1_df['EntityType'].tolist()\n",
    "    entity2ids.extend(entity2d)\n",
    "    entity2dtypes.extend(entity2dtype)\n",
    "    \n",
    "    entity1d = entity_type1_df['EntityUniqueId'].tolist()\n",
    "    entity1dtype = entity_type1_df['EntityType'].tolist()\n",
    "    entity1ids.extend(entity1d)\n",
    "    entity1dtypes.extend(entity1dtype)\n",
    "    entity2d = entity_type3_df['EntityUniqueId'].tolist()\n",
    "    entity2dtype = entity_type3_df['EntityType'].tolist()\n",
    "    entity2ids.extend(entity2d)\n",
    "    entity2dtypes.extend(entity2dtype)\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------\n",
    "    \n",
    "    return entity1ids, entity2ids, entity1dtypes, entity2dtypes, merged_df\n",
    "\n",
    "\n",
    "def get_columns():\n",
    "    columns = ['DocId', 'Entity1Id', 'Entity2Id', 'Entity1Type', 'Entity2Type', 'Date']\n",
    "    return columns\n",
    "\n",
    "\n",
    "def get_columns2():\n",
    "    columns = ['DocId', 'EventId', 'Entity1Id', 'Entity2Id', 'Date']\n",
    "    return columns\n",
    "\n",
    "\n",
    "def get_event_id2(event_mentions_df):\n",
    "    event_ids = []\n",
    "    event_id = ''\n",
    "    for i in range(len(event_mentions_df)):\n",
    "        if event_mentions_df['Entity2Type'][i] == 2:\n",
    "            event_id = 1\n",
    "        elif event_mentions_df['Entity2Type'][i] == 3:\n",
    "            event_id = 1\n",
    "        else:\n",
    "            event_id = 1\n",
    "        event_ids.append(event_id)\n",
    "    return event_ids\n",
    "\n",
    "\n",
    "def clean_na(event_mentions_df2, merged_df):\n",
    "    column_name = event_mentions_df2.columns.to_list()\n",
    "    working_df = event_mentions_df2.merge(merged_df, left_on='Entity2Id', right_on='EntityUniqueId',\n",
    "                                          suffixes=('', '_x'))\n",
    "    working_df = working_df.dropna(subset=['EntityName'])\n",
    "    working_df = working_df[working_df['EntityName'] != ''].reset_index(drop=True)\n",
    "    working_df2 = working_df[column_name]\n",
    "    return working_df2, working_df\n",
    "\n",
    "\n",
    "def get_relations3(connected_entity_df, event_mentions_df2):\n",
    "    column_name = event_mentions_df2.columns.to_list()\n",
    "    connected_entity_df = connected_entity_df[connected_entity_df.duplicated(subset='EntityName', keep=False)]\n",
    "    connected_entity_df['Entitiy1Id_copy'] = connected_entity_df['Entity1Id']\n",
    "    connected_entity_df['Entitiy2Id_copy'] = connected_entity_df['Entity2Id']\n",
    "    connected_entity_df['Entity1Id'] = connected_entity_df['Entitiy2Id_copy']\n",
    "    connected_entity_df['Entity2Id'] = connected_entity_df['Entitiy1Id_copy']\n",
    "    connected_entity_df['EventId'] = 2\n",
    "    connected_entity_df = connected_entity_df[column_name]\n",
    "    event_mentions_df2 = event_mentions_df2.append(connected_entity_df)\n",
    "    event_mentions_df2 = event_mentions_df2.drop_duplicates(subset=['Entity1Id', 'Entity2Id']).reset_index(drop=True)\n",
    "\n",
    "    return event_mentions_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "deef2e97-c762-4bc5-97e5-5e19fc80ca97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 62.50it/s]\n",
      "C:\\Users\\DataMicron\\AppData\\Local\\Temp\\ipykernel_19424\\934985324.py:12: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  datalake_df['JUMLAH URUSNIAGA'] = datalake_df['JUMLAH URUSNIAGA'].str.replace('+', '')\n"
     ]
    }
   ],
   "source": [
    "# Append all excel files into a single table\n",
    "datalake_df = pd.DataFrame()\n",
    "dates_df = pd.DataFrame()\n",
    "for i in tqdm(range(len(file_list))):\n",
    "    # Read the Input database\n",
    "    transaction_df = pd.read_excel(file_list[i])[['JUMLAH URUSNIAGA', 'SENDER', 'RECEIVER']]\n",
    "    datalake_date = pd.read_excel(file_list[i])[['TARIKH MASUK']]\n",
    "    datalake_df = datalake_df.append(transaction_df,ignore_index=True)\n",
    "    dates_df = dates_df.append(datalake_date,ignore_index=True)\n",
    "\n",
    "# Clean column 'JUMLAH URUSNIAGA'\n",
    "datalake_df['JUMLAH URUSNIAGA'] = datalake_df['JUMLAH URUSNIAGA'].str.replace('+', '')\n",
    "datalake_df['JUMLAH URUSNIAGA'] = datalake_df['JUMLAH URUSNIAGA'].str.replace('-', '')\n",
    "datalake_df['JUMLAH URUSNIAGA'] = datalake_df['JUMLAH URUSNIAGA'].str.replace(',', '')\n",
    "datalake_df['JUMLAH URUSNIAGA'] = datalake_df['JUMLAH URUSNIAGA'].astype('float')\n",
    "datalake_df['JUMLAH URUSNIAGA'] = datalake_df['JUMLAH URUSNIAGA'].apply(lambda x: f\"RM {x}\")\n",
    "\n",
    "# Transpose Documents from Datalake into EagleyeDB\n",
    "column_name = ['DocId','DocName','DocLocation','Date','Text']\n",
    "document_df = pd.DataFrame({column_name[0]:[1], column_name[1]:['AUDIT DATABASES'],column_name[2]:['AUDIT SYSTEM/DATABASES'],\n",
    "                            column_name[3]:['1/1/2022  12:00:00 AM'],column_name[4]:[' ']})\n",
    "document_df['Date'] = document_df['Date'].astype('datetime64')\n",
    "\n",
    "# Transpose Entity Type from Datalake into EagleyeDB\n",
    "column_name = ['TypeId', 'TypeName', 'TypeDescription']\n",
    "entity_type = get_entity_type(datalake_df)\n",
    "entity_type_desc = get_entity_desc(entity_type)\n",
    "entity_type_id = get_entity_type_id(datalake_df)\n",
    "entity_type_df2 = pd.DataFrame([entity_type_id, entity_type, entity_type_desc]).T\n",
    "entity_type_df2.columns = column_name\n",
    "# print(entity_type_df)\n",
    "\n",
    "# Transpose EntityAttribute from Datalake into Eagle Eye DB\n",
    "column_name = ['EntityId','AttributeName','AttributeValue','DocId']\n",
    "entity_attributes_df = pd.DataFrame({column_name[0]:[''],column_name[1]:[''],column_name[2]:[''],column_name[3]:['']})\n",
    "\n",
    "# Transpose Entities from Datalake into Eagle Eye DB\n",
    "column_name = ['EntityId','EntityName','EntityType','Location','Address','Image','DocId']\n",
    "entities = get_entity(datalake_df)\n",
    "entity_id = get_entity_id(entities)\n",
    "factor = get_factor2(entity_id)\n",
    "entity_type2 = get_entity_type2(entity_id, entity_type_id)\n",
    "location = get_location(factor)\n",
    "address = get_address(factor)\n",
    "image = get_image(entity_type2)\n",
    "doc_id = get_doc_id(factor)\n",
    "searcharound = get_search_around(entity_type2)\n",
    "searcharoundurl = get_search_around_url(factor)\n",
    "entity_df2 = pd.DataFrame([entity_id, entities, entity_type2, location, address, image, doc_id]).T\n",
    "entity_df2.columns = column_name\n",
    "entity_df2['EntityName'] = entity_df2['EntityName'].astype('string')\n",
    "entity_df2 = entity_df2.fillna('')\n",
    "entity_df2['EntityName'] = entity_df2['EntityName'].str.replace(' 00:00:00', '')\n",
    "# print(entity_df)\n",
    "\n",
    "\n",
    "# Transpose Entity Mentions from Datalake into Eagle Eye DB\n",
    "column_name = ['DocId','EntityId','StartIndex','EndIndex','Date','EntityUniqueId']\n",
    "doc_id = get_doc_id2(entity_id)\n",
    "entity_ids = get_entity_id2(entity_id)\n",
    "start_idx = get_start_idx(entity_id)\n",
    "end_idx = get_end_idx(entity_id)\n",
    "date = get_date(dates_df)\n",
    "entity_unique_id = get_entity_unique_id(doc_id, entity_ids, start_idx, end_idx)\n",
    "entity_mentions_df2 = pd.DataFrame([doc_id, entity_ids, start_idx, end_idx, date, entity_unique_id]).T\n",
    "entity_mentions_df2.columns = column_name\n",
    "entity_mentions_df2['Date'] = entity_mentions_df2['Date'].astype('datetime64')\n",
    "# print(entity_mentions_df)\n",
    "\n",
    "\n",
    "# Transpose Event Types from Datalake into Eagle Eye DB\n",
    "column_name = ['EventId','EventName','EventDescription']\n",
    "event_name = get_event_name()\n",
    "event_desc = get_event_desc(event_name)\n",
    "event_id = get_event_id(event_name)\n",
    "event_types_df2 = pd.DataFrame([event_id, event_name, event_desc]).T\n",
    "event_types_df2.columns = column_name\n",
    "# print(event_types_df2)\n",
    "\n",
    "\n",
    "# Transpose Event Mentions from Datalake into Eagle Eye DB\n",
    "entity1id, entity2id, entity_type1d, entity_type2d, merged_df = get_relations(entity_df2, entity_mentions_df2)\n",
    "doc_id = get_doc_id2(entity1id)\n",
    "date = get_date2(dates_df)\n",
    "event_mentions_df2 = pd.DataFrame([doc_id, entity1id, entity2id, entity_type1d, entity_type2d, date]).T\n",
    "column_name = get_columns()\n",
    "event_mentions_df2.columns = column_name\n",
    "event_mentions_df2['Date'] = event_mentions_df2['Date'].astype('datetime64')\n",
    "event_id = get_event_id2(event_mentions_df2)\n",
    "event_mentions_df2['EventId'] = event_id\n",
    "column_name = get_columns2()\n",
    "event_mentions_df2 = event_mentions_df2[column_name]\n",
    "# event_mentions_df2\n",
    "\n",
    "\n",
    "# Auto-connect among entities with attributes\n",
    "# event_mentions_df2, connected_entity_df = clean_na(event_mentions_df2, merged_df)\n",
    "# event_mentions_df2 = get_relations3(connected_entity_df, event_mentions_df2)\n",
    "# print(event_mentions_df2)\n",
    "\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('EagleyeSampleDb.xlsx', engine='openpyxl')\n",
    "\n",
    "# Write each dataframe to a different worksheet.\n",
    "document_df.to_excel(writer, sheet_name='Documents', index=False)\n",
    "entity_type_df2.to_excel(writer, sheet_name='EntityTypes', index=False)\n",
    "entity_df2.to_excel(writer, sheet_name='Entities', index=False)\n",
    "entity_attributes_df.to_excel(writer, sheet_name='EntityAttributes', index=False)\n",
    "entity_mentions_df2.to_excel(writer, sheet_name='EntityMentions', index=False)\n",
    "event_types_df2.to_excel(writer, sheet_name='EventTypes', index=False)\n",
    "event_mentions_df2.to_excel(writer, sheet_name='EventMentions', index=False)\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef1652-b80f-4e6b-bc5b-7b877b06502a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
